Converting Raw files into Matlab format 
Pretraining a deep autoencoder. 
The Science paper used 50 epochs. This uses   0 
Reducing num batches to 50. 
Pretraining Layer 1 with RBM: 784-500 

Pretraining Layer 2 with RBM: 500-500 

Pretraining Layer 3 with RBM: 500-2000 

Training discriminative model on MNIST by minimizing cross entropy error. 
60 batches of 1000 cases each. 
Before epoch 1 Train # misclassified: 54077 (from 60000). Test # misclassified: 9020 (from 10000) 	 	 
Before epoch 2 Train # misclassified: 12337 (from 60000). Test # misclassified: 1962 (from 10000) 	 	 
